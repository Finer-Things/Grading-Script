{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe03cc38",
   "metadata": {},
   "source": [
    "# Math 34A Spring 2022 Grading Script\n",
    "\n",
    "## Import Libraries and Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6a89616",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from grading_functions import *\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a332a70a",
   "metadata": {},
   "source": [
    "## Import Grade Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb908ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"grades is from Gradescope, \n",
    "roster is the current student list from egrades, \n",
    "webwork is the homework total downloaded from Gauchospace\"\"\"\n",
    "grades = pd.read_csv(\"Math_34A_Spring_2022_Grades.csv\")\n",
    "roster = pd.read_csv(\"Math_34A_Spring_2022_Roster.csv\")\n",
    "webwork = pd.read_csv(\"Math_34A_Spring_2022_Webwork.csv\", na_values=[\"-\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84f998b",
   "metadata": {},
   "source": [
    "## Creating a Quiz Max Points List\n",
    "The list will be used later to adjust quiz grades for students who were sick and missed section. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f277c3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Total Quiz Points Column\n",
    "quiz_point_max_columns = [name for name in grades.columns if \"Max Points\" in name and \"quiz\" in name.lower()]\n",
    "#grades[\"QPTot\"] = grades.apply(lambda row: sum([row[name] for name in quiz_point_max_columns]), axis=1)\n",
    "quiz_max_points = grades[quiz_point_max_columns].iloc[1].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4fbcdd",
   "metadata": {},
   "source": [
    "## Basic Cleaning\n",
    "1 Renaming columns in all three dataframes for compatability and ease of reading\n",
    "\n",
    "2 Deleting garbage columns from Gradescope's list\n",
    "\n",
    "3 Slimming down the egrades roster and webwork roster to include only what we need for student identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b07ec9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Renameing \"Perm #\" to \"SID\" in Egrades, which may not even be necessary. \n",
    "roster.rename(columns = {\"Perm #\": \"SID\"}, inplace = True)\n",
    "\n",
    "#Renaming the same and the Homework column in Webwork\n",
    "webwork.rename(columns = {\"ID number\": \"SID\"}, inplace = True)\n",
    "webwork.rename(columns = {\"External tool:Webwork (Percentage)\": \"HWT\"}, inplace = True)\n",
    "\n",
    "#Deleting Garbage Columns\n",
    "junk_column_indices = [i for i, column in enumerate(grades.columns) \\\n",
    "                       if \"Max Points\" in column or \"Submission Time\" in column \\\n",
    "                       or \"Lateness\" in column or column == \"section_name\" or column == \"Email\"]\n",
    "grades.drop(grades.columns[junk_column_indices], axis=1, inplace=True)\n",
    "\n",
    "#Renaming Columns: Homework and quizzes (In grades df)\n",
    "quiz_column_names = [name for name in grades.columns if \"quiz\" in name.lower()]\n",
    "grades.rename(columns = {name: \"Quiz\"+\" \"+str(i+1) for i, name in enumerate(quiz_column_names)}, inplace = True)\n",
    "\n",
    "\n",
    "#Roster and Webwork Operations\n",
    "roster = roster[[\"Enrl Cd\", \"SID\"]]\n",
    "webwork = webwork[[\"SID\", \"HWT\"]]\n",
    "\n",
    "#Cleaning Webwork's Homework Total Column because there are % symbols, making it an object. \n",
    "#Note: The commented-out method on two lines below worked, but on the line below it I tried using methods I learned \n",
    "#from Codecademy on 6/30 that also work but with a simpler regex command and a more robust .to_numeric pandas method. \n",
    "#webwork[\"HWT\"] = webwork[\"HWT\"].str.extract('([0-9.]+)').astype(float)\n",
    "webwork[\"HWT\"] = pd.to_numeric(webwork[\"HWT\"].str.replace(\"[\\%]\", \"\", regex=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b79974",
   "metadata": {},
   "source": [
    "## Merging the two versions of Midterm 1\n",
    "They are merged into a new column  called \"Midterm 1\" and dropped once this is done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf80a90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "midterm1_version_list = [\"Midterm 1 Gold\", \"Midterm 1 Blue\"]\n",
    "grades[\"Midterm 1\"] = grades.apply(version_merger, \n",
    "                                   version_list=midterm1_version_list, \n",
    "                                   axis=1)\n",
    "#grades[\"Midterm 1\"] = grades.apply(lambda row: version_merger(row, midterm1_version_list), axis=1)\n",
    "grades.drop(columns=midterm1_version_list, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56147114",
   "metadata": {},
   "source": [
    "## Merging the Three Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b335d8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#merging Gradescope and Egrades\n",
    "grades_merge_one = pd.merge(grades, roster, on=\"SID\", how =\"right\")\n",
    "#merging the result and Webwork\n",
    "grades_for_submission = pd.merge(webwork, grades_merge_one, on=\"SID\", how =\"right\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3041d2",
   "metadata": {},
   "source": [
    "## More pruning of columns\n",
    "I'm not sure if this is necessary because it looks like the same was done for the Gradescope file. Maybe it's also done for the egrades list - and this could be done at the beginning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fbb824c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pruning more columns from general roster\n",
    "junk_column_indices = [i for i, column in enumerate(grades_for_submission.columns) \\\n",
    "                       if \"Pronouns\" in column or \"Lateness\" in column \\\n",
    "                       or column == \"section_name\" or column == \"Email\"]\n",
    "grades_for_submission.drop(grades_for_submission.columns[junk_column_indices], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c187f87e",
   "metadata": {},
   "source": [
    "## Scaling the Midterm Scores to make them out of 100. \n",
    "Their column names are also shortened to make the spreadsheet easier to read when you open the file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd4ddb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scaling the Midterms\n",
    "grades_for_submission[\"M1\"] = np.round(grades_for_submission[\"Midterm 1\"]*4, decimals=1)\n",
    "grades_for_submission[\"M2\"] = np.round(100*grades_for_submission[\"Midterm 2\"]/23, decimals=1)\n",
    "grades_for_submission[\"M3\"] = np.round(100*grades_for_submission[\"Midterm 3\"]/40, decimals=1)\n",
    "#Drop Midterm Point Columns\n",
    "grades_for_submission.drop(columns=[\"Midterm 1\", \"Midterm 2\", \"Midterm 3\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78afebb0",
   "metadata": {},
   "source": [
    "## Adjusting Quiz Grades to account for excused absences. \n",
    "\n",
    "### Part 1 \n",
    "Getting a list of all of Trevor's students so all of their quiz 4's and quiz 6's can be excused. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9eec720c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Missing Quiz Corrections\n",
    "#Trevor's cover left all of the quizzes out on a table in the tea room, \n",
    "#so here is a list of all of his students. \n",
    "trevor_df = pd.read_csv(\"Excused Tests/Trevor's Students.csv\")\n",
    "trevor_students = list(trevor_df[\"SID\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf5fcef",
   "metadata": {},
   "source": [
    "## Adjusting Quiz Grades to account for excused absences.\n",
    "\n",
    "### Part 2 \n",
    "Constructing a dataframe to hold information in the place where a student has an excused abscence from a quiz: The row/column corresponding to that quiz score for that student has a tuple. Students with excused absences have a (0,max_points) tuple and every other entry has a (score, 0) tuple. The quiz average column of the dataframe is then computed using these tuples: We sum over the scores of the first entries (giving the total quiz points earned) and divided by the total possible quiz points minus the individual max_point values for quizzes that are excused. This computes the same total as if the students had their excused quizzes replaced by their quiz average, and we'll use this fact at the bottom of the cell below (though it's not needed for grade calculation). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a293532d",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_quiz_matrix = grades_for_submission[[\"SID\"]].copy()\n",
    "#excused_list_points\n",
    "n=len(quiz_max_points)\n",
    "for i in range(1,n+1):\n",
    "    quiz_df = pd.read_csv(\"Excused Tests/Quiz \"+str(i)+\" Excused Students.csv\")\n",
    "    excused_list = list(quiz_df[\"SID\"])\n",
    "    missing_quiz_matrix[\"Quiz \"+str(i)+\"-\"] = grades_for_submission.fillna(0).apply(quiz_tuple_function, \n",
    "                                                                                    excused_list = excused_list, \n",
    "                                                                                    trevor_students = trevor_students, \n",
    "                                                                                    i = i, \n",
    "                                                                                    quiz_max_points = quiz_max_points, \n",
    "                                                                                    axis=1)\n",
    "\n",
    "    \n",
    "\n",
    "#Computing the Quiz Total (average percentage score)\n",
    "grades_for_submission[\"QT\"] = missing_quiz_matrix.apply(quiz_average_calculator, \n",
    "                                                        i = i, \n",
    "                                                        quiz_max_points = quiz_max_points, \n",
    "                                                        axis=1)\n",
    "\n",
    "#Just for fun, this replaces the excused missing quiz score with quiz total percent of max points, so if you add the\n",
    "#entries in Excel, you should get the same quiz total for these students. I just tried it in Excel. They agree up to \n",
    "#a rounding error. \n",
    "for i in range(1,len(quiz_max_points)+1):\n",
    "    quiz_df = pd.read_csv(\"Excused Tests/Quiz \"+str(i)+\" Excused Students.csv\")\n",
    "    excused_list = list(quiz_df[\"SID\"])\n",
    "    grades_for_submission[\"Quiz \"+str(i)] = grades_for_submission.apply(quiz_missing_points_allocator, \n",
    "                                                                        quiz_max_points = quiz_max_points, \n",
    "                                                                        i = i, \n",
    "                                                                        excused_list = excused_list, \n",
    "                                                                        trevor_students = trevor_students, \n",
    "                                                                        axis=1\n",
    "                                                                       )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d865720d",
   "metadata": {},
   "source": [
    "## Sorting the Remaining Columns\n",
    "If I open a CSV file of the df now, columns are everywhere! This block sorts the remaining columns and puts the key ones in the front. \n",
    "\n",
    "The dataframe with grades on it is also shortened from grades_for_submission to gfs since I copied it. \n",
    "The copy was probably not necessary, but I was new to the practice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0bdcb1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's sort the columns and shorten the name of the data set (so gfs will be grades_for_submission sorted)\n",
    "gfs_copy = grades_for_submission.copy()\n",
    "#first_columns = [\"First Name\", \"Last Name\", \"Enrl Cd\", \"SID\", \"Grade\", \"ClassLevel\", \"Major1\", \"Major2\"]\n",
    "first_columns = [\"First Name\", \"Last Name\", \"Enrl Cd\", \"SID\"]\n",
    "gfs_copy.drop(first_columns, axis=1, inplace=True)\n",
    "gfs_copy = gfs_copy[sorted(gfs_copy.columns)]\n",
    "gfs = gfs_copy.copy()\n",
    "for i, info_col in enumerate(first_columns):\n",
    "    gfs.insert(i, info_col, grades_for_submission[info_col], True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4c6b44",
   "metadata": {},
   "source": [
    "## New Columns: QT and QTA\n",
    "Creating a Quiz Total column and an Adjusted Quiz Total column to account for the Quiz Reflections: x% of half their missing points should be added, where x is their score on the corresponding \"Reflection for Improvement\" assignment on Gradescope. For example, if they reflected on 100% of their missed questions, then half of the difference between their quiz average and 100% would be added to their quiz total. For example, an initial quiz total of 40% would be boosted to 70% for a student who completed the reflection. \n",
    "\n",
    "### Remark: \n",
    "In Gradescope I named the reflection assignment \"Q Reflections...\" instead of \"Quiz Reflections...\" because otherwise the script above would have renamed it something like \"Quiz 11.\" Clearly, we don't want that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b4fa48db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Quiz Total Column\n",
    "quiz_list=[\"Quiz \"+str(i) for i in range(1,len(quiz_column_names)+1)]\n",
    "missing_quiz_point_multiplier = gfs[\"Q Reflection for Improvement\"].fillna(0)/200\n",
    "gfs[\"QTA\"] = gfs[\"QT\"] + (100-gfs[\"QT\"])*missing_quiz_point_multiplier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac8e67d",
   "metadata": {},
   "source": [
    "## Final Exam Column\n",
    "Converting the \"Final Exam\" raw score into a \"Final\" entry with a percent value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "58f7d553",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_point_total = 72\n",
    "gfs[\"Final\"] = np.round(gfs[\"Final Exam\"]*100/final_point_total, decimals=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c67e7b",
   "metadata": {},
   "source": [
    "## Replacing Excused Missing Midterms \n",
    "These will be replaced with the final percentage (that was just calculated). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "783c2121",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replacing Midterm Entries for Excused Absences (that midterm is replaced w/ the final score)\n",
    "for i in range(1,4):\n",
    "    excused_df = pd.read_csv(\"Excused Tests/Midterm \"+str(i)+\" Excused Students.csv\")\n",
    "    students_list = list(excused_df[\"SID\"])\n",
    "    gfs[\"M\"+str(i)] = gfs.apply(lambda row: np.round(missed_exam_replacer(row,\n",
    "                                                                             students_list, \n",
    "                                                                            \"M\"+str(i), \n",
    "                                                                            \"Final\"), \n",
    "                                                    decimals=1),\n",
    "                                axis=1)\n",
    "\n",
    "#Final Replacement will be done below because we need to replace the final with the midterm average"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e52b379",
   "metadata": {},
   "source": [
    "## Computing the Midterm Average\n",
    "MT or Midterm Total is the raw percentage as the average of the two highest midterms, which is congruent with making the third midterm optional so students could take a third midterm if they wanted to improve their midterm grade. In practice, I added the three midterm percentages, subtracted dropped the minimum, and then divided by two. I could not find a way to just \"drop the lowest,\" though I strongly suspect there is a numpy or Pandas method for this. \n",
    "\n",
    "MTA is the adjusted midterm average, with x% of the missing points returned based on the x% completed for the midterm reflections assignment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d7256bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Two New Midterm Grade Columns (Raw Average and \"MT\" that drops the lowest midterm) -- I dropped \"Raw Average\" below\n",
    "gfs[\"Raw Midterm Average\"] = gfs.apply(lambda row: (row.fillna(0)[\"M1\"]+row.fillna(0)[\"M2\"]+row.fillna(0)[\"M3\"])/3\n",
    "                                                                     , axis = 1)\n",
    "gfs[\"MT\"] = gfs.apply(lambda row: (3*row[\"Raw Midterm Average\"] - np.min([row.fillna(0)[\"M1\"],row.fillna(0)[\"M2\"],\n",
    "                                                                          row.fillna(0)[\"M3\"]]))/2, axis = 1)\n",
    "#Adjusting the Midterm Total to include missing points\n",
    "missing_midterm_point_multiplier = gfs[\"Exam Reflection for Improvement\"].fillna(0)/200\n",
    "gfs[\"MTA\"] = gfs[\"MT\"] + (100-gfs[\"MT\"])*missing_midterm_point_multiplier\n",
    "gfs.drop(columns=[\"Raw Midterm Average\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2c83b4",
   "metadata": {},
   "source": [
    "## Excused Absence Replacement\n",
    "There were some students who had documented sickness during the final exam. The code below used the list of SIDs sent to me by my TAs and based on that list, replaced those students' final scores with their adjusted midterm average to avoid them having to take the course as an incomplete. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b4f2e0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replacing Final Entries for Excused Absences (that midterm is replaced w/ the final score)\n",
    "final_df = pd.read_csv(\"Excused Tests/Final Exam Excused Students.csv\")\n",
    "final_missed = list(final_df[\"SID\"])\n",
    "gfs[\"Final\"] = gfs.apply(lambda row: missed_exam_replacer(row, final_missed, \"Final\", \"MTA\"), \n",
    "                                axis=1)\n",
    "#gfs[\"Final na0\"] = gfs[\"Final\"].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00790e5c",
   "metadata": {},
   "source": [
    "## Computing the \"Grade\" and \"Letter Grade\" columns, now that we're ready. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fb62e42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grade Column\n",
    "gfs[\"Grade\"] = gfs.apply(lambda row: .1*row[\"QTA\"] + .2*row[\"HWT\"] + .4*row[\"MTA\"] + .3*row.fillna(0)[\"Final\"] + 3,\n",
    "                        axis=1)\n",
    "   \n",
    "\n",
    "#Letter Grade Column\n",
    "gfs[\"Letter Grade\"] = gfs.apply(lambda row: letter_grade_assigner(row), axis=1)\n",
    "\n",
    "\n",
    "#ordering columns for Egrades Submission and my sanity\n",
    "first_columns = [\"SID\", \"Enrl Cd\", \"Letter Grade\", \"Final\", \"MTA\", \"HWT\", \"QTA\"]\n",
    "temp_columns = gfs[first_columns]\n",
    "gfs.drop(columns=first_columns, axis=1, inplace=True)\n",
    "for (i, col_name) in enumerate(first_columns):\n",
    "    gfs.insert(i, col_name, temp_columns[col_name])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50592f0",
   "metadata": {},
   "source": [
    "## Saving the finished dataframe as a CSV file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a264bfcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save gfs df to a file\n",
    "gfs.to_csv('Math_34A_Winter_2022_Grades_python_output.csv', header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38e1be6",
   "metadata": {},
   "source": [
    "## Creating a slimmer CSV file for Uploading to Egrades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "863a9a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Slim Format for Submission\n",
    "\"Enrl Cd\", \"SID\"\n",
    "gfs_slim = gfs[[\"Enrl Cd\", \"SID\", \"Letter Grade\"]]\n",
    "gfs_slim.to_csv('Math 34A Egrades List Spring 22.csv', header=True, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
